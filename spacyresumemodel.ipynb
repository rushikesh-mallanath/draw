{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebb7bdd-1551-498f-a199-3139c8349842",
      "metadata": {
        "id": "7ebb7bdd-1551-498f-a199-3139c8349842"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.high_level import extract_text\n",
        "from spacy.matcher import Matcher\n",
        "import pandas as pd\n",
        "import pdfkit\n",
        "import docx2pdf\n",
        "import doc2pdf\n",
        "import win32com.client as win32\n",
        "from win32com.client import constants\n",
        "import os,os.path\n",
        "import PyPDF2\n",
        "import fitz\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44aa6791-142f-493b-bcba-c0d44afe8921",
      "metadata": {
        "id": "44aa6791-142f-493b-bcba-c0d44afe8921"
      },
      "outputs": [],
      "source": [
        "#1 converting code\n",
        "\n",
        "import os\n",
        "from shutil import move\n",
        "from django.shortcuts import render, redirect\n",
        "from .models import Document\n",
        "from django.contrib import messages\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "from docx2pdf import convert as docx2pdf_convert\n",
        "import comtypes.client\n",
        "import win32com.client\n",
        "import comtypes\n",
        "\n",
        "# I'm converting docx to pdf\n",
        "def convert_docx_to_pdf(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    import comtypes\n",
        "\n",
        "    # Initialize COM\n",
        "    comtypes.CoInitialize()\n",
        "    word = comtypes.client.CreateObject(\"Word.Application\")\n",
        "    for root, _, files in os.walk(input_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".docx\"):\n",
        "                input_file = os.path.join(root, filename)\n",
        "                pdf_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.pdf\")\n",
        "\n",
        "                # Convert .docx to PDF using comtypes\n",
        "                doc = word.Documents.Open(input_file)\n",
        "                doc.SaveAs(pdf_file, FileFormat=17)  # FileFormat 17 corresponds to PDF\n",
        "                doc.Close()\n",
        "    word.Quit()\n",
        "\n",
        "#I'm converting doc to pdf\n",
        "def convert_doc_to_pdf(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "        # Initialize COM\n",
        "    comtypes.CoInitialize()\n",
        "\n",
        "    word = win32com.client.DispatchEx(\"Word.Application\")\n",
        "    for root, _, files in os.walk(input_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".doc\"):\n",
        "                input_file = os.path.join(root, filename)\n",
        "                pdf_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.pdf\")\n",
        "\n",
        "                # Convert .doc to PDF using pywin32 (Word Automation)\n",
        "                doc = word.Documents.Open(input_file)\n",
        "                doc.SaveAs(pdf_file, FileFormat=17)  # FileFormat 17 corresponds to PDF\n",
        "                doc.Close()\n",
        "    word.Quit()\n",
        "\n",
        "\n",
        "def index(request):\n",
        "    if request.method == \"POST\":\n",
        "        documents = request.FILES.getlist(\"documents\")\n",
        "        unsupported_files = []\n",
        "        for document in documents:\n",
        "            # Check if it's a PDF file\n",
        "            is_pdf = document.name.lower().endswith(\".pdf\")\n",
        "\n",
        "            # Define the target directory based on file type\n",
        "            if is_pdf:\n",
        "                target_directory = r\"C:\\Users\\alex\\Downloads\\Output\"\n",
        "            else:\n",
        "                target_directory = r\"C:\\Users\\alex\\Downloads\\input\"\n",
        "\n",
        "            if not document.name.lower().endswith((\".doc\", \".docx\", \".pdf\")):\n",
        "                unsupported_files.append(document.name)\n",
        "                continue\n",
        "\n",
        "            # Move the uploaded file to the target directory\n",
        "            file_path = os.path.join(target_directory, document.name)\n",
        "            with open(file_path, 'wb') as destination:\n",
        "                for chunk in document.chunks():\n",
        "                    destination.write(chunk)\n",
        "\n",
        "            # Perform the conversion if it's not a PDF\n",
        "            if not is_pdf:\n",
        "                # convert_doc_and_docx_to_pdf(target_directory, r\"C:\\Users\\alex\\Downloads\\Output\")\n",
        "                convert_docx_to_pdf(target_directory, r\"C:\\Users\\alex\\Downloads\\Output\")\n",
        "                convert_doc_to_pdf(target_directory, r\"C:\\Users\\alex\\Downloads\\Output\")\n",
        "            # Create a Document object (optional)\n",
        "            Document.objects.create(document=file_path)\n",
        "        if unsupported_files:\n",
        "            messages.error(request, \"File format not supported for: \" + \", \".join(unsupported_files))\n",
        "        else:\n",
        "            messages.success(request, \"Documents Uploaded and Processed\")\n",
        "    return render(request, \"index.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882b4dde-646c-4a71-9b70-9eb822e6ef31",
      "metadata": {
        "id": "882b4dde-646c-4a71-9b70-9eb822e6ef31"
      },
      "outputs": [],
      "source": [
        "pip install docx2pdf PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f331aab-2f7b-4b0b-9f80-dc9cb3b7be3d",
      "metadata": {
        "id": "1f331aab-2f7b-4b0b-9f80-dc9cb3b7be3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import comtypes.client\n",
        "\n",
        "input_folder_path = r\"C:\\Users\\alex\\Downloads\\Input\"\n",
        "output_folder_path = r\"C:\\Users\\alex\\Downloads\\Output\"\n",
        "def convert_docx_to_pdf(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for root, _, files in os.walk(input_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".docx\"):\n",
        "                input_file = os.path.join(root, filename)\n",
        "                pdf_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.pdf\")\n",
        "\n",
        "                # Convert .docx to PDF using comtypes\n",
        "                word = comtypes.client.CreateObject(\"Word.Application\")\n",
        "                doc = word.Documents.Open(input_file)\n",
        "                doc.SaveAs(pdf_file, FileFormat=17)  # FileFormat 17 corresponds to PDF\n",
        "                doc.Close()\n",
        "                word.Quit()\n",
        "\n",
        "def convert_doc_to_pdf(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for root, _, files in os.walk(input_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".doc\"):\n",
        "                input_file = os.path.join(root, filename)\n",
        "                pdf_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.pdf\")\n",
        "\n",
        "                # Convert .doc to PDF using comtypes\n",
        "                word = comtypes.client.CreateObject(\"Word.Application\")\n",
        "                doc = word.Documents.Open(input_file)\n",
        "                doc.SaveAs(pdf_file, FileFormat=17)  # FileFormat 17 corresponds to PDF\n",
        "                doc.Close()\n",
        "                word.Quit()\n",
        "\n",
        "def convert_doc_and_docx_to_pdf(input_folder, output_folder):\n",
        "    convert_docx_to_pdf(input_folder, output_folder)\n",
        "    convert_doc_to_pdf(input_folder, output_folder)\n",
        "    print(\"Done\")\n",
        "\n",
        "convert_doc_and_docx_to_pdf(input_folder_path, output_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e41618-1044-4ebc-acb9-458c98c4c8fc",
      "metadata": {
        "id": "c1e41618-1044-4ebc-acb9-458c98c4c8fc"
      },
      "outputs": [],
      "source": [
        "#webdev\n",
        "\n",
        "<!doctype html>\n",
        "<html lang=\"en\">\n",
        "  <head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "    <title>BULK RESUME UPLOADER</title>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65\" crossorigin=\"anonymous\">\n",
        "  </head>\n",
        "  <body>\n",
        "    <div class=\"container mt-5 border rounded p-5 bg-light shadow\" style=\"width:400px\">\n",
        "        {% if messages %}\n",
        "            {% for message in messages %}\n",
        "                <div class=\"text-center alert alert-success mb-3\">{{message}}</div>\n",
        "            {% endfor %}\n",
        "        {% endif %}\n",
        "        <h3 class=\"text-center\">Upload Resume</h3>\n",
        "        <form action=\"\" method=\"post\" class=\"mt-3\" enctype=\"multipart/form-data\">\n",
        "            {% csrf_token %}\n",
        "            <div class=\"mt-2\">\n",
        "                <label for=\"documents\" style=\"font-size: 13px;\">Documents</label>\n",
        "                <br>\n",
        "                <input required multiple type=\"file\" name=\"documents\" id=\"documents\" class=\"form-control\">\n",
        "            </div>\n",
        "            <div class=\"d-grid mt-5\">\n",
        "                <button class=\"btn btn-primary\">Submit</button>\n",
        "            </div>\n",
        "        </form>\n",
        "        <div class=\"progress mt-3\" style=\"display: none;\">\n",
        "          <div class=\"progress-bar\" role=\"progressbar\" style=\"width: 0%;\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\">0%</div>\n",
        "        </div>\n",
        "    </div>\n",
        "  </body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b2b82e-d7ac-4bdb-94e2-08d67a6c3901",
      "metadata": {
        "id": "a5b2b82e-d7ac-4bdb-94e2-08d67a6c3901"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Function to extract name using spaCy\n",
        "def extract_name(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    nlp_text = nlp(text)\n",
        "\n",
        "    # Define a pattern for extracting names (two consecutive proper nouns)\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "\n",
        "    matcher.add('NAME', [pattern])\n",
        "\n",
        "    matches = matcher(nlp_text)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        if 'name' not in span.text.lower():\n",
        "            return span.text\n",
        "\n",
        "# Function to extract mobile number using regex\n",
        "def extract_mobile_number(text):\n",
        "    phone = re.findall(re.compile(\n",
        "        r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'),\n",
        "        text)\n",
        "\n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if len(number) >= 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number\n",
        "\n",
        "# Function to extract email using regex\n",
        "def extract_email(text):\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None\n",
        "\n",
        "# Function to extract skills\n",
        "def extract_skills(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    # Load skills data from CSV\n",
        "    skills_csv_path = r\"C:\\Users\\alex\\Downloads\\skills.csv\"\n",
        "    skills_db_csv_path = r\"C:\\Users\\alex\\Downloads\\skills_db.csv\"\n",
        "\n",
        "    if os.path.isfile(skills_csv_path) and os.path.isfile(skills_db_csv_path):\n",
        "        data1 = pd.read_csv(skills_csv_path)\n",
        "        data2 = pd.read_csv(skills_db_csv_path)\n",
        "        data = pd.concat([data1, data2], ignore_index=True)\n",
        "        skills = list(data.columns.values)\n",
        "    else:\n",
        "        print(\"CSV files not found. Please make sure the files exist at the specified paths.\")\n",
        "        return []\n",
        "\n",
        "    # Process text to extract skills\n",
        "    nlp_text = nlp(text)\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "\n",
        "    skillset = []\n",
        "\n",
        "    # Check for one-grams\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "\n",
        "    # Check for bi-grams and tri-grams\n",
        "    for chunk in nlp_text.noun_chunks:\n",
        "        chunk_text = chunk.text.lower().strip()\n",
        "        if chunk_text in skills:\n",
        "            skillset.append(chunk_text)\n",
        "\n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
        "\n",
        "# Main function to process PDF files\n",
        "def main():\n",
        "    output_directory = r\"C:\\Users\\alex\\Downloads\\Output\"  # Directory containing PDF files\n",
        "\n",
        "    # Create a list to store results for each PDF\n",
        "    results = []\n",
        "\n",
        "    # Iterate over PDF files in the output directory\n",
        "    for filename in os.listdir(output_directory):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(output_directory, filename)\n",
        "\n",
        "            # Extract text from the PDF file\n",
        "            print(f\"Extracting text from {filename}...\")\n",
        "            text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "            # Extract name, email, mobile number, and skills\n",
        "            name = extract_name(text)\n",
        "            email = extract_email(text)\n",
        "            mobile_number = extract_mobile_number(text)\n",
        "            skills = extract_skills(text)\n",
        "\n",
        "            # Store the results in a dictionary\n",
        "            result = {\n",
        "                \"Filename\": filename,\n",
        "                \"Name\": name,\n",
        "                \"Email\": email,\n",
        "                \"Mobile Number\": mobile_number,\n",
        "                \"Skills\": skills\n",
        "            }\n",
        "\n",
        "            # Append the result to the list\n",
        "            results.append(result)\n",
        "\n",
        "    # Print the results\n",
        "    for result in results:\n",
        "        print(f\"Resume Name: {result['Filename']}\")\n",
        "        print(f\"Name: {result['Name']}\")\n",
        "        print(f\"Email: {result['Email']}\")\n",
        "        print(f\"Mobile Number: {result['Mobile Number']}\")\n",
        "        print(f\"Skills: {', '.join(result['Skills'])}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d5e7c1-eb94-4302-bb11-0b4d61addfe0",
      "metadata": {
        "id": "03d5e7c1-eb94-4302-bb11-0b4d61addfe0"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc110b1e-d29f-4214-b095-0c4eee1d7927",
      "metadata": {
        "id": "fc110b1e-d29f-4214-b095-0c4eee1d7927"
      },
      "outputs": [],
      "source": [
        " lst = \"asdfghhjaklqwertzxvcb\"\n",
        "match = re.findall(r\"a\", lst)\n",
        "\n",
        "print(match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc346c7-0997-4c25-b147-5fb05ca8c68d",
      "metadata": {
        "id": "9cc346c7-0997-4c25-b147-5fb05ca8c68d"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"C:\\Users\\alex\\Downloads\\testcv\\testres.pdf\"\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77aa1e4-a30c-4f83-973e-c5aeaca4b581",
      "metadata": {
        "id": "a77aa1e4-a30c-4f83-973e-c5aeaca4b581"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"C:\\Users\\alex\\Downloads\\Output\\resume1.pdf\"\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2779e118-d78c-44b0-a135-eec7d48576a9",
      "metadata": {
        "id": "2779e118-d78c-44b0-a135-eec7d48576a9"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"C:\\Users\\alex\\Downloads\\Output\\testdoc_resume4.pdf\"\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4062724-728c-4380-84da-49526914d570",
      "metadata": {
        "id": "b4062724-728c-4380-84da-49526914d570"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"C:\\Users\\alex\\Downloads\\Output\\testdoc2-resume.pdf\"\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcc2fe8-3afe-4ed3-9095-04b02a97d301",
      "metadata": {
        "id": "dfcc2fe8-3afe-4ed3-9095-04b02a97d301"
      },
      "outputs": [],
      "source": [
        "pdf_path = r\"C:\\Users\\alex\\Downloads\\Output\\testdoc5_resume.pdf\"\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with fitz.open(pdf_path) as pdf_document:\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "extract_text_from_pdf(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b999b9ad-2159-4d24-8964-f3b9c556fc32",
      "metadata": {
        "id": "b999b9ad-2159-4d24-8964-f3b9c556fc32"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_experience_and_domain(text):\n",
        "    # Regular expression patterns to match years of experience and domain\n",
        "    experience_pattern = r'(\\d+) years of experience'\n",
        "    domain_pattern = r'(Office Administrator|DATA ENGINEER|FULL STACK PYTHON DEVELOPER|Amazon Associate|SOFTWARE ENGINEER|Warehouse Associate|Laboratory Inventory Assistant)'\n",
        "\n",
        "    # Search for years of experience and domain in the resume text\n",
        "    experience_match = re.search(experience_pattern, resume1_text)\n",
        "    domain_match = re.search(domain_pattern, resume1_text)\n",
        "\n",
        "    # Initialize variables to store the results\n",
        "    years_of_experience = None\n",
        "    domain = None\n",
        "\n",
        "    # Extract years of experience if found\n",
        "    if experience_match:\n",
        "        years_of_experience = int(experience_match.group(1))\n",
        "\n",
        "    # Extract domain if found\n",
        "    if domain_match:\n",
        "        domain = domain_match.group(1)\n",
        "\n",
        "    return years_of_experience, domain\n",
        "\n",
        "# Example usage:\n",
        "# resume1_text = \"Jason Miller\\nAmazon Associate\\nProfile\\nExperienced Amazon Associate with five years’ tenure in a shipping yard...\"\n",
        "# resume2_text = \"Omkar Pathak\\nSOFTWARE ENGINEER · FULL STACK PYTHON DEVELOPER\\nPune, Maharashtra, India...\"\n",
        "# resume3_text = \"Highly organized and detail-oriented office manager with over 5 years of experience managing administrative tasks...\"\n",
        "\n",
        "years_exp1, domain1 = extract_experience_and_domain(resume1_text)\n",
        "# years_exp2, domain2 = extract_experience_and_domain(resume2_text)\n",
        "# years_exp3, domain3 = extract_experience_and_domain(resume3_text)\n",
        "\n",
        "print(f\"Resume 1: {years_exp1} years of experience in {domain1}\")\n",
        "# print(f\"Resume 2: {years_exp2} years of experience in {domain2}\")\n",
        "# print(f\"Resume 3: {years_exp3} years of experience in {domain3}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77115512-47e3-4b66-95a2-d82e359f73da",
      "metadata": {
        "id": "77115512-47e3-4b66-95a2-d82e359f73da"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "resume_text1 = \"NOVEMBER 3, 2019 \\nOMKAR PATHAK · RÉSUMÉ \\n1 \\n \\nOmkar Pathak \\nSOFTWARE ENGINEER · FULL STACK PYTHON DEVELOPER \\nPune, Maharashtra, India \\n(+91) 8087996634 \\n| \\nomkarpathak27@gmail.com \\n| \\nwww.omkarpathak.in \\n|     \\n OmkarPathak \\n|     \\n omkar-pathak-94473811b \\n \\n“Make the change that you want to see in the world.” \\n \\nExperience   \\n \\nSchlumberger \\nPune, Maharashtra, India \\nDATA ENGINEER \\nJuly 2018 - Present \\n• Responsible for implementing and managing an end-to-end CI/CD Pipeline with custom validations for Informatica migrations which \\nbrought migration time to 1.5 hours from 9 hours without any manual intervention \\n• Enhancing, auditing and maintaining custom data ingestion framework that ingest around 1TB of data each day to over 70 business \\nunits \\n• Working with L3 developer team to ensure the discussed Scrum PBI’s are delivered on time for data ingestions \\n• Planning and Executing QA and Production Release Cycle activities \\nTruso \\nPune, Maharashtra, India \\nFULL STACK DEVELOPER INTERN \\nJune 2018 - July 2018 \\n• Created RESTful apis \\n• Tried my hands on Angular 5/6 \\n• Was responsible for Django backend development \\nPropeluss \\nPune, Maharashtra, India \\nDATA ENGINEERING INTERN \\nOctober 2017 - January 2018 \\n• Wrote various automation scripts to scrape data from various websites. \\n• Applied Natural Language Processing to articles scraped from the internet to extract different entities in these articles using entity \\nextraction algorithms and applying Machine Learning to classify these articles. \\n• Also applied KNN with LSA for extracting relevant tags for various startups based on their works. \\nGeeksForGeeks \\nPune, Maharashtra, India \\nTECHNICAL CONTENT WRITER \\nJuly 2017 - September 2017 \\n• Published 4 articles for the topics such as Data Structures and Algorithms and Python \\nSofttestlab Technologies \\nPune, Maharashtra, India \\nWEB DEVELOPER INTERN \\nJune 2017 - July 2017 \\n• Was responsible for creating an internal project for the company using PHP and Laravel for testing purposes \\n• Worked on a live project for creating closure reports using PHP and Excel \\n \\nProjects   \\n \\nPyresparser \\nAPI/Python Package \\nPERSONAL PROJECT \\nJuly 2019 - Present \\n• A simple resume parser used for extracting information from resumes \\n• Extract information from thousands of resumes in just a few seconds \\n• Author and maintainer of this project \\nGarbage Level Monitoring System \\nIoT \\nTEAM PROJECT \\nOctober 2017 - May 2018 \\n• To find a economical and smarter alternative to current garbage problems \\n• Users can monitor levels of all garbage bins from a global dashboard provided \\n• Was responsible for Django backend development \\nNOVEMBER 3, 2019 \\nOMKAR PATHAK · RÉSUMÉ \\n2 \\n \\nPygorithm \\nAPI / Python Package \\nPERSONAL PROJECT \\nJuly 2017 - Present \\n• Author and maintainer of this project \\n• An educational library to teach all the major algorithms \\n• Got covered in Fosstack, FullStackFeed, Kleiber and Tagged under Hotest Github Project on ITCodeMonkey \\n \\n \\n \\nSmart Surveillance System using Raspberry Pi and Face Recognition \\nIoT \\nPERSONAL PROJECT \\nJanuary 2017 - February 2017 \\n• Face Recognition using OpenCV and Python \\n• Raspberry Pi was used as the data server \\n• User notified if any suspicious activity detected in real time \\n \\nPassword Strength Evaluator using Machine Learning \\nMachine Learning \\nPERSONAL PROJECT \\nMarch 2017 \\n• SVM algorithm used for training and classification \\n• Flask framework used \\n• Self-generated dataset \\n \\nEducation   \\n \\nMarathwada Mitra Mandal’s College of Engineering \\nPune, Maharashtra, India \\nB.E. IN COMPUTER ENGINEERING \\n2014 - 2018 \\n• Aggregate 74% \\nSkills   \\n \\n \\nProgramming Languages: Python, C, PHP, C++, Shell Script \\nFrontend Technologies: HTML, CSS, JavaScript, Angular 6/7 \\nBackend Technologies: Django, Flask (Python), Laravel (PHP) \\nOperating Systems:  Linux, Unix, Windows \\nDatabases: MySQL, SQLite, MongoDB \\nOther:  Git, NLP, Scikit-Learn, OpenCV, Cloud (GCP, Azure, DigitalOcean) \\n \\nHonors & Awards   \\n \\n \\n2018 Top rated Python developer, in Pune and Fifth in India at Github \\nIndia \\n2018 Quora Top Writer, \\nIndia \\n2018 Awarded ‘The Best Outgoing Student Award 2017-18’, \\nMMCOE, Pune \\n2018 Won 2nd Prize, in an Hackathon organized by MIT-ADT Persona Fest 2018 \\nPune \\n \\n2018 \\nBest Paper Award, in National Level Conference on “Emerging Trends in Computing , Analytics \\nand Security - 2018”(NCETCAS-2018) \\nMMCOE, Pune \\n \\nExtracurricular Activities   \\n \\nContributor in Pune PyCon 2018 \\nPUNE, MAHARASHTRA, INDIA \\n2018 \\n• Was a part of Website Designing and volunteering committee \\nNOVEMBER 3, 2019 \\nOMKAR PATHAK · RÉSUMÉ \\n3 \\n \\nMentor at GirlScript Summer of Code 2019 \\nPUNE, MAHARASHTRA, INDIA \\n2019 \\n• Mentored 4+ teams in various domains \\nOrganizing head for the National level technical event - \\nInnovatus \\nPUNE, MAHARASHTRA, INDIA \\n2018 \\n• Organized project competitions \\n \\nWorkshop on IoT and Python \\nMMCOE, PUNE \\n10 Jan 2017 \\n• Conducted a workshop for second year students to give them a brief overview about IoT by completing three mini projects and taught \\nthem basics of Python programming language \\n \\nPublications   \\n \\n \\nSmart Surveillance System using Raspberry Pi and Face \\nRecognition \\n \\nGarbage Level Monitoring System \\n \\nDOI10.17148/IJARCCE.2017.64117 \\n \\nInterests   \\n \\n• Competitive Programming \\n• Photography \\n• Sketching \\n• Reading/Writing on Quora \\n• Contributing to Open Source projects \\n\"\n",
        "\n",
        "def calculate_experience(date_ranges):\n",
        "    today = datetime.now()\n",
        "    total_experience = 0\n",
        "\n",
        "    for start_date, end_date in date_ranges:\n",
        "        if end_date.lower() == 'present':\n",
        "            end_date = today\n",
        "        else:\n",
        "            end_date = datetime.strptime(end_date, \"%B %Y\")\n",
        "\n",
        "        start_date = datetime.strptime(start_date, \"%B %Y\")\n",
        "        experience = (end_date - start_date).days / 365  # Calculate experience in years\n",
        "        total_experience += experience\n",
        "\n",
        "    return total_experience\n",
        "\n",
        "def extract_experience_from_resume(resume_text):\n",
        "    # Split the text into lines\n",
        "    lines = resume_text.split('\\n')\n",
        "\n",
        "    # Initialize variables to store sections and date ranges\n",
        "    current_section = ''\n",
        "    date_ranges = []\n",
        "\n",
        "    # Define a function to check if a line is a section header\n",
        "    def is_section_header(line):\n",
        "        return re.match(r'(Experience|Employment History)', line, re.IGNORECASE)\n",
        "\n",
        "    for line in lines:\n",
        "        if is_section_header(line):\n",
        "            # Start a new section\n",
        "            if current_section:\n",
        "                # Extract date ranges from the current section\n",
        "                experience_dates = re.findall(r'(\\w+ \\d{4}) — (\\w+ \\d{4}|\\w+ present)', current_section)\n",
        "                date_ranges.extend(experience_dates)\n",
        "            current_section = line\n",
        "        else:\n",
        "            # Add the line to the current section\n",
        "            current_section += '\\n' + line\n",
        "\n",
        "    # Extract date ranges from the last section\n",
        "    if current_section:\n",
        "        experience_dates = re.findall(r'(\\w+ \\d{4}) — (\\w+ \\d{4}|\\w+ present)', current_section)\n",
        "        date_ranges.extend(experience_dates)\n",
        "\n",
        "    # Calculate total experience in years\n",
        "    total_experience = calculate_experience(date_ranges)\n",
        "\n",
        "    return total_experience\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "experience = extract_experience_from_resume(resume_text)\n",
        "print(f\"Total Experience: {experience:.2f} years\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6487911f-27b4-4285-9707-8018eb0869e2",
      "metadata": {
        "id": "6487911f-27b4-4285-9707-8018eb0869e2"
      },
      "outputs": [],
      "source": [
        "import docx2txt\n",
        "from pdfminer.high_level import extract_text\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44754c97-4a43-41a7-a641-3344a659f567",
      "metadata": {
        "id": "44754c97-4a43-41a7-a641-3344a659f567",
        "outputId": "d9e67a2d-d535-465d-8442-fe8e8b5490a6"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\alex\\\\Downloads\\\\testcv\\\\JD.docx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m job_discription \u001b[38;5;241m=\u001b[39m \u001b[43mdocx2txt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43malex\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtestcv\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mJD.docx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m resume \u001b[38;5;241m=\u001b[39m docx2txt\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124malex\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtestcv\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtestdoc_resume4.docx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/my-jupyter-env/lib/python3.8/site-packages/docx2txt/docx2txt.py:76\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(docx, img_dir)\u001b[0m\n\u001b[1;32m     73\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# unzip the docx in memory\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m zipf \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m filelist \u001b[38;5;241m=\u001b[39m zipf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# get header text\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# there can be 3 header files in the zip\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.8/zipfile.py:1251\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\alex\\\\Downloads\\\\testcv\\\\JD.docx'"
          ]
        }
      ],
      "source": [
        "job_discription = docx2txt.process(r'C:\\Users\\alex\\Downloads\\testcv\\JD.docx')\n",
        "resume = docx2txt.process(r'C:\\Users\\alex\\Downloads\\testcv\\testdoc_resume4.docx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e4187-3da2-4c87-94ea-3efbe37e7d3b",
      "metadata": {
        "id": "3f9e4187-3da2-4c87-94ea-3efbe37e7d3b"
      },
      "outputs": [],
      "source": [
        "print(resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a4cf5f7-1940-4df6-817c-1c36edf450c9",
      "metadata": {
        "id": "9a4cf5f7-1940-4df6-817c-1c36edf450c9"
      },
      "outputs": [],
      "source": [
        "content = [job_discription, resume]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b2d15d-df78-4906-b451-4303574ce479",
      "metadata": {
        "id": "57b2d15d-df78-4906-b451-4303574ce479"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "matrix = cv.fit_transform(content)\n",
        "\n",
        "##\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b424b9f4-00c8-4551-a89a-c7f106a68625",
      "metadata": {
        "id": "b424b9f4-00c8-4551-a89a-c7f106a68625"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix1 = cosine_similarity(matrix)\n",
        "\n",
        "##\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b621943-3a1c-4e38-b202-738bed17645a",
      "metadata": {
        "id": "6b621943-3a1c-4e38-b202-738bed17645a"
      },
      "outputs": [],
      "source": [
        "!pip uninstall nvidia-cublas-cu11\n",
        "#nvidia-cuda-cupti-cu11\n",
        "# nvidia-cuda-nvrtc-cu11    11.7.99\n",
        "# nvidia-cuda-runtime-cu11  11.7.99\n",
        "# nvidia-cudnn-cu11         8.5.0.96\n",
        "# nvidia-cufft-cu11         10.9.0.58\n",
        "# nvidia-curand-cu11        10.2.10.91\n",
        "# nvidia-cusolver-cu11      11.4.0.1\n",
        "# nvidia-cusparse-cu11      11.7.4.91\n",
        "# nvidia-nccl-cu11          2.14.3\n",
        "# nvidia-nvtx-cu11          11.7.91"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08eb787-391f-4785-8937-3f342263e437",
      "metadata": {
        "id": "f08eb787-391f-4785-8937-3f342263e437"
      },
      "outputs": [],
      "source": [
        "print(similarity_matrix1)\n",
        "print(similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7821cba3-b7da-44bb-a3b7-2b7cbf7b0a3d",
      "metadata": {
        "id": "7821cba3-b7da-44bb-a3b7-2b7cbf7b0a3d",
        "outputId": "9280f974-b29a-46f8-e96d-b6ef221efe55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in ./lib/python3.8/site-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./lib/python3.8/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./lib/python3.8/site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./lib/python3.8/site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./lib/python3.8/site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./lib/python3.8/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./lib/python3.8/site-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./lib/python3.8/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./lib/python3.8/site-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./lib/python3.8/site-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./lib/python3.8/site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in ./lib/python3.8/site-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./lib/python3.8/site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./lib/python3.8/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in ./lib/python3.8/site-packages (from spacy) (1.24.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./lib/python3.8/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./lib/python3.8/site-packages (from spacy) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in ./lib/python3.8/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./lib/python3.8/site-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./lib/python3.8/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.3 in ./lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in ./lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.8/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: spacy_transformers in ./lib/python3.8/site-packages (1.2.5)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in ./lib/python3.8/site-packages (from spacy_transformers) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in ./lib/python3.8/site-packages (from spacy_transformers) (1.24.4)\n",
            "Requirement already satisfied: transformers<4.31.0,>=3.4.0 in ./lib/python3.8/site-packages (from spacy_transformers) (4.30.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in ./lib/python3.8/site-packages (from spacy_transformers) (2.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in ./lib/python3.8/site-packages (from spacy_transformers) (2.4.7)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in ./lib/python3.8/site-packages (from spacy_transformers) (0.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./lib/python3.8/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in ./lib/python3.8/site-packages (from torch>=1.8.0->spacy_transformers) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in ./lib/python3.8/site-packages (from torch>=1.8.0->spacy_transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in ./lib/python3.8/site-packages (from torch>=1.8.0->spacy_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in ./lib/python3.8/site-packages (from torch>=1.8.0->spacy_transformers) (3.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.8.0->spacy_transformers)\n",
            "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Requirement already satisfied: triton==2.0.0 in ./lib/python3.8/site-packages (from torch>=1.8.0->spacy_transformers) (2.0.0)\n",
            "Requirement already satisfied: wheel in ./lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->spacy_transformers) (0.34.2)\n",
            "Requirement already satisfied: cmake in ./lib/python3.8/site-packages (from triton==2.0.0->torch>=1.8.0->spacy_transformers) (3.27.4.1)\n",
            "Requirement already satisfied: lit in ./lib/python3.8/site-packages (from triton==2.0.0->torch>=1.8.0->spacy_transformers) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./lib/python3.8/site-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (0.17.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.8/site-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./lib/python3.8/site-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (2023.8.8)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./lib/python3.8/site-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in ./lib/python3.8/site-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (0.3.3)\n",
            "Requirement already satisfied: fsspec in ./lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy_transformers) (2023.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.3 in ./lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.8/site-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./lib/python3.8/site-packages (from sympy->torch>=1.8.0->spacy_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11\n",
            "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "!pip install spacy_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc2e501-13e5-4253-abed-ff49c0a1b6af",
      "metadata": {
        "id": "edc2e501-13e5-4253-abed-ff49c0a1b6af"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19195dbd-2013-4769-b8dd-3e5e4db2a583",
      "metadata": {
        "id": "19195dbd-2013-4769-b8dd-3e5e4db2a583",
        "outputId": "09d3b0ec-7ea5-4364-e83c-1af7ecf35d32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.6.1'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3736a196-285b-4b33-b0e0-b839cae4b1dd",
      "metadata": {
        "id": "3736a196-285b-4b33-b0e0-b839cae4b1dd"
      },
      "outputs": [],
      "source": [
        "cv_data = json.load(open(r'/home/rushikeshmallanath/Downloads/spacyResume/dataset.json', 'r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c39a9e1-e29c-433e-84bb-b3c2f61a7613",
      "metadata": {
        "id": "8c39a9e1-e29c-433e-84bb-b3c2f61a7613",
        "outputId": "e16bc55a-8591-4172-a219-3b5232caaa8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1014"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(cv_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bba4bb5-2d3b-494c-b6bb-3eff5915be0b",
      "metadata": {
        "id": "9bba4bb5-2d3b-494c-b6bb-3eff5915be0b",
        "outputId": "b689104d-aa15-4edf-a806-a806102494c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\xa0 \\xa0\\nContact\\nwww.linkedin.com/in/omjagri\\n(LinkedIn)\\nTop Skills\\nphp\\nMySQL\\nJavaScript\\nCertifications\\nPhp & Js Om Prakash Jagri\\nFull Stack Developer | PHP | Laravel | Vue Js\\nKathmandu, Bāgmatī, Nepal\\nSummary\\nExperienced Developer with a demonstrated history of working in\\nthe information technology and services industry. Skilled in Laravel,\\nPHP, Cascading Style Sheets (CSS), JavaScript, vue js and MySQL.\\nStrong engineering professional with a B.sc.CSIT(Bachelors of\\nScience in Computer Science and Information Technology) focused\\nin Computer Science from Tribhuvan University, Institute of Science\\n& Tchnology. \\nExperience\\nSearchable Design LLC\\nSoftware Developer\\nJune 2021\\xa0-\\xa0Present\\xa0 (1 year 7 months)\\nNepal\\nFull Stack Developer Laravel with Vue Js\\nBenekiva\\nTechnical Documentation\\nSeptember 2021\\xa0-\\xa0Present\\xa0 (1 year 4 months)\\nUnited States\\nBidhee\\n3 years 9 months\\nLaravel Developer\\nMarch 2018\\xa0-\\xa0May 2021\\xa0 (3 years 3 months)\\nBaneswar Kathamandu\\nInternship\\nSeptember 2017\\xa0-\\xa0February 2018\\xa0 (6 months)\\nKathamandu Nepal\\nWeb Development in PHP/JS With Laravel Framework \\nEducation\\n\\xa0 Page 1 of 2\\xa0 \\xa0\\nSiddhanath Science Campus Mahendranagar\\nB.sc.CSIT(Bachelors of Science in Computer Science and Information\\nTechnology),\\xa0Computer Science \\xa0·\\xa0(2013\\xa0-\\xa02018)\\n\\xa0 Page 2 of 2',\n",
              " {'entities': [[12, 39, 'LINKEDIN LINK'],\n",
              "   [62, 65, 'SKILLS'],\n",
              "   [66, 71, 'SKILLS'],\n",
              "   [72, 82, 'SKILLS'],\n",
              "   [98, 106, 'CERTIFICATION'],\n",
              "   [107, 123, 'NAME'],\n",
              "   [147, 150, 'SKILLS'],\n",
              "   [153, 160, 'SKILLS'],\n",
              "   [163, 169, 'SKILLS'],\n",
              "   [170, 195, 'LOCATION'],\n",
              "   [622, 643, 'COMPANIES WORKED AT'],\n",
              "   [644, 662, 'WORKED AS'],\n",
              "   [685, 700, 'YEARS OF EXPERIENCE'],\n",
              "   [749, 757, 'COMPANIES WORKED AT'],\n",
              "   [758, 781, 'WORKED AS'],\n",
              "   [809, 824, 'YEARS OF EXPERIENCE'],\n",
              "   [840, 846, 'COMPANIES WORKED AT'],\n",
              "   [864, 881, 'WORKED AS'],\n",
              "   [906, 922, 'YEARS OF EXPERIENCE'],\n",
              "   [944, 954, 'WORKED AS'],\n",
              "   [988, 996, 'YEARS OF EXPERIENCE'],\n",
              "   [1092, 1131, 'COLLEGE NAME'],\n",
              "   [1132, 1228, 'DEGREE']]}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5932c5-7d09-490e-addd-c76c8ec1fd3d",
      "metadata": {
        "id": "fc5932c5-7d09-490e-addd-c76c8ec1fd3d",
        "outputId": "617d5d9c-4ff8-4c4b-da5b-8f2b16e42105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/home/rushikeshmallanath/Downloads/spacyResume/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config /home/rushikeshmallanath/Downloads/spacyResume/base_config.cfg  /home/rushikeshmallanath/Downloads/spacyResume/config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311dfeac-ea0a-4a24-9ad2-baa18780ee9b",
      "metadata": {
        "id": "311dfeac-ea0a-4a24-9ad2-baa18780ee9b"
      },
      "outputs": [],
      "source": [
        "def get_spacy_doc(file, data):\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    db = DocBin()\n",
        "\n",
        "    for text, annot in tqdm(data):\n",
        "        doc = nlp.make_doc(text)\n",
        "        annot = annot['entities']\n",
        "\n",
        "        ents = []\n",
        "        entity_indices = []\n",
        "\n",
        "        for start, end, label in annot:\n",
        "            skip_entity = False\n",
        "            for idx in range(start, end):\n",
        "                if idx in entity_indices:\n",
        "                    skip_entity = True\n",
        "                    break\n",
        "            if skip_entity == True:\n",
        "                continue\n",
        "\n",
        "            entity_indices = entity_indices + list(range(start, end))\n",
        "\n",
        "            try:\n",
        "                span = doc.char_span(start, end, label = label, alignment_mode = 'strict')\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if span is None:\n",
        "                err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
        "                file.write(err_data)\n",
        "            else:\n",
        "                ents.append(span)\n",
        "\n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d543a17-77b0-405f-b32c-8f64529474c2",
      "metadata": {
        "id": "5d543a17-77b0-405f-b32c-8f64529474c2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143a680f-963e-40c2-8b55-4d41c735ebee",
      "metadata": {
        "id": "143a680f-963e-40c2-8b55-4d41c735ebee",
        "outputId": "04f11a1b-1d82-46ea-c055-94e115a48fdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(709, 305)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa0a2d7-1f14-4baf-9822-950d60a1b923",
      "metadata": {
        "id": "afa0a2d7-1f14-4baf-9822-950d60a1b923",
        "outputId": "7191622e-1d1f-43ed-c5cc-7ec715146846"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████| 709/709 [00:04<00:00, 146.94it/s]\n",
            "100%|████████████████████████████████| 305/305 [00:01<00:00, 154.45it/s]\n"
          ]
        }
      ],
      "source": [
        "file = open(r'/home/rushikeshmallanath/Downloads/spacyResume/train_file.txt', 'w')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk(r'/home/rushikeshmallanath/Downloads/spacyResume/train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk(r'/home/rushikeshmallanath/Downloads/spacyResume/test_data.spacy')\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dfce04f-35bf-4d78-837e-1b9ae7e04e94",
      "metadata": {
        "id": "2dfce04f-35bf-4d78-837e-1b9ae7e04e94",
        "outputId": "8e0184fd-8ead-4a4f-99a3-31a1dcf9e8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "/home/rushikeshmallanath/Downloads/spacyResume/output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    137.15    0.00    0.00    0.00    0.00\n",
            "  0     200       4349.42  14445.46   46.59   51.32   42.66    0.47\n",
            "  0     400       2056.43   9226.96   53.54   62.13   47.04    0.54\n",
            "  0     600        854.38   7123.34   56.18   53.50   59.13    0.56\n",
            "  1     800        865.67   4723.51   72.05   72.16   71.93    0.72\n",
            "  1    1000        512.00   4349.79   73.42   75.67   71.30    0.73\n",
            "  1    1200        548.29   4256.58   75.17   81.21   69.97    0.75\n",
            "  1    1400        333.86   3594.59   69.71   77.86   63.10    0.70\n",
            "  2    1600        437.44   3242.25   64.22   67.30   61.41    0.64\n",
            "  2    1800        655.66   3152.80   77.87   80.11   75.76    0.78\n",
            "  2    2000        690.95   3883.81   76.71   76.21   77.21    0.77\n",
            "  3    2200        529.77   3139.05   78.07   79.96   76.27    0.78\n",
            "  3    2400        566.18   2915.00   79.71   83.02   76.65    0.80\n",
            "  3    2600        673.24   3257.13   77.80   79.29   76.37    0.78\n",
            "  3    2800        539.60   2981.98   79.07   81.43   76.84    0.79\n",
            "  4    3000        960.90   3052.35   74.40   77.23   71.78    0.74\n",
            "  4    3200        736.30   3020.58   78.24   77.25   79.26    0.78\n",
            "  4    3400        790.27   2804.12   79.55   82.24   77.03    0.80\n",
            "  5    3600        766.34   2789.46   80.98   82.14   79.85    0.81\n",
            "  5    3800        775.39   3035.47   80.65   82.38   78.99    0.81\n",
            "  5    4000       1655.42   3004.23   80.01   80.41   79.61    0.80\n",
            "  6    4200        774.77   2979.46   79.75   83.22   76.55    0.80\n",
            "  6    4400       2487.00   3451.33   81.69   84.07   79.45    0.82\n",
            "  7    4600        792.84   2978.83   81.00   83.70   78.47    0.81\n",
            "  7    4800       1319.49   3993.50   80.42   80.03   80.81    0.80\n",
            "  8    5000       1472.82   3870.81   80.16   79.35   80.98    0.80\n",
            "  9    5200       1167.01   3509.74   81.74   84.78   78.92    0.82\n",
            "  9    5400        957.90   3354.89   80.81   83.27   78.50    0.81\n",
            " 10    5600       1196.78   3464.39   81.76   84.66   79.05    0.82\n",
            " 10    5800       1007.08   3274.75   80.91   83.51   78.46    0.81\n",
            " 11    6000       1091.03   3197.05   79.90   80.53   79.29    0.80\n",
            " 11    6200       3861.95   3149.38   80.85   81.72   79.99    0.81\n",
            " 12    6400        919.98   2940.98   81.47   80.94   82.02    0.81\n",
            " 13    6600        894.86   2671.43   81.70   81.76   81.64    0.82\n",
            " 13    6800        951.87   2701.53   81.68   81.58   81.79    0.82\n",
            " 14    7000       1303.26   2760.54   80.18   80.68   79.69    0.80\n",
            " 14    7200        901.66   2489.18   81.13   83.03   79.31    0.81\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/home/rushikeshmallanath/Downloads/spacyResume/output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train /home/rushikeshmallanath/Downloads/spacyResume/config.cfg  --output /home/rushikeshmallanath/Downloads/spacyResume/output --paths.train /home/rushikeshmallanath/Downloads/spacyResume/train_data.spacy --paths.dev /home/rushikeshmallanath/Downloads/spacyResume/test_data.spacy --gpu-id -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c3c5bb-ef26-4a9a-8a2a-fe8bfa27b41e",
      "metadata": {
        "id": "97c3c5bb-ef26-4a9a-8a2a-fe8bfa27b41e"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('/home/rushikeshmallanath/Downloads/spacyResume/output/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e06802-701b-472c-a497-d885848ca41c",
      "metadata": {
        "id": "b3e06802-701b-472c-a497-d885848ca41c",
        "outputId": "ccbb78f5-31cf-4d7d-a066-a0f77ccd2fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Obtaining dependency information for PyMuPDF from https://files.pythonhosted.org/packages/3a/d2/a6042d0dc90a609e487169031f0af5d789acb01d8a8f73ba31fffcfe225d/PyMuPDF-1.23.3-cp38-none-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading PyMuPDF-1.23.3-cp38-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.23.3 (from PyMuPDF)\n",
            "  Obtaining dependency information for PyMuPDFb==1.23.3 from https://files.pythonhosted.org/packages/34/cf/bb6a3bad0ffdea1fb9fcae53de6c1b9187746c32e17377ff8f36b9d98ee4/PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
            "  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading PyMuPDF-1.23.3-cp38-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.3 PyMuPDFb-1.23.3\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b767245-ff1a-4f62-a88f-df240bce1d0b",
      "metadata": {
        "id": "7b767245-ff1a-4f62-a88f-df240bce1d0b"
      },
      "outputs": [],
      "source": [
        "import sys, fitz\n",
        "\n",
        "fname = '/home/rushikeshmallanath/Downloads/BCBS1.pdf'\n",
        "doc = fitz.open(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e387027-6b6d-4417-95de-d62fcb0c85ae",
      "metadata": {
        "id": "7e387027-6b6d-4417-95de-d62fcb0c85ae"
      },
      "outputs": [],
      "source": [
        "text = \" \"\n",
        "for page in doc:\n",
        "    text = text + str(page.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91321aa-565e-43e9-9e04-7ab6ac2fd8e7",
      "metadata": {
        "id": "c91321aa-565e-43e9-9e04-7ab6ac2fd8e7",
        "outputId": "75be1210-cd6e-4d89-b66d-768f6948832a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m(text)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ent\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m, ent\u001b[38;5;241m.\u001b[39mlabel_)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" - \", ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be9c207c-e865-4c55-bbaa-62b9a9fc3b81",
      "metadata": {
        "id": "be9c207c-e865-4c55-bbaa-62b9a9fc3b81"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}